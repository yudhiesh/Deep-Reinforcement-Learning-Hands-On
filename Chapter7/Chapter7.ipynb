{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om13TfR4fw-m"
      },
      "source": [
        "# Higher-Level RL Libraries\n",
        "\n",
        "## PTAN\n",
        "\n",
        "### Action Selecters\n",
        "\n",
        "An action selecter is an object that helps with going from network output to concrete action values.\n",
        "\n",
        "-> **Argmax** used by Q-value methods when the network predicts Q-values for a set of actions and trhe desired action is the action with the larges *Q(s,a)*. \n",
        "\n",
        "-> **Policy-based** where the network ouptuts the probablity distribution and an action needs to be sampled from this distribution. \n",
        "\n",
        "An action selecter is used by the Agent and rarely needs to be customized but you have this option. Concrete classes provided by the library are:\n",
        "\n",
        "-> **ArgmaxActionSelecrtor** which applies argmax on the second axis of a passed tensor. (It assumes a matrix with batch dimension along the first axis)\n",
        "\n",
        "-> **ProbabilityActionSelector** which samples from the probability distribution of a discrete set of actions\n",
        "\n",
        "-> **EpsilonGreedyActionSelecter** has the parameter epsilon which specifies the probability of a random action to be taken\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6rk4cQ4hlGg",
        "outputId": "db09c0e8-4f94-41e1-86ff-7349dd389f94"
      },
      "source": [
        "!pip3 install ptan"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ptan\n",
            "  Downloading https://files.pythonhosted.org/packages/d9/0b/c93ddb49b9f291062d1d3f63efd3d7e6614749214d15c8d8af2211d1b220/ptan-0.7.tar.gz\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from ptan) (1.7.0+cu101)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from ptan) (0.17.3)\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (from ptan) (0.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ptan) (1.19.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from ptan) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->ptan) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->ptan) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->ptan) (3.7.4.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py->ptan) (1.15.0)\n",
            "Building wheels for collected packages: ptan\n",
            "  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptan: filename=ptan-0.7-cp36-none-any.whl size=23502 sha256=6ca73266235078f169df26ce4b6122f58643170b2e57767e5a5e74029fa6802c\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/58/0c/a42dad12a5cc0e130453042707b3e2205adfb901ae35cfad75\n",
            "Successfully built ptan\n",
            "Installing collected packages: ptan\n",
            "Successfully installed ptan-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I49DHB1hK0jr"
      },
      "source": [
        "import numpy as np \n",
        "import ptan"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWvSqN28hed0",
        "outputId": "813de1c2-99ea-4ea8-be22-3a49d80e803f"
      },
      "source": [
        "q_vals = np.array([[1,2,3], [1,-1,0]])\n",
        "q_vals"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [ 1, -1,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSY5bDuGh9cN",
        "outputId": "524a2067-d8c9-4c11-e318-12efee0ebda4"
      },
      "source": [
        "selector = ptan.actions.ArgmaxActionSelector()\n",
        "selector(q_vals)\n",
        "# returns the indices of the outputs with the largest value |"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCMY1WqmiFCE",
        "outputId": "54f7d37f-f7e9-428b-c146-3bc2f3435f95"
      },
      "source": [
        "selector_epsilon = ptan.actions.EpsilonGreedyActionSelector(epsilon=1)\n",
        "selector_epsilon(q_vals)\n",
        "# With epsilon set to 0 we get the arg max of the actions as there are no random actions taken\n",
        "# With epsilon set to 1 we get completely random actions that are taken which is why the values are different"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVKq-CJ_iVWk",
        "outputId": "191c70f7-9ed4-4973-93f2-aa1ad544c860"
      },
      "source": [
        "# The input for the probability distribution needs to be a normalized probability distribution\n",
        "# [0.1,0.8, 0.1] in the probability distribution represents the probabilities of each actions\n",
        "# in the first one the action at index 1 will have the highest probability \n",
        "# [1 2 0] -> outputs the action that is taken by the index\n",
        "# the first probability distribution is used for this and the resulting ones are used for the other ones\n",
        "# the second example the 2nd index action is the one with the highest probability\n",
        "# the third example the 1st and the 2nd index have the same probability which is why it could be either action 0 or action 1 for this \n",
        "selector_prob = ptan.actions.ProbabilityActionSelector()\n",
        "for _ in range(10):\n",
        "  acts = selector_prob(np.array([\n",
        "                                 [0.1,0.8, 0.1],\n",
        "                                 [0.0, 0.0,1.0],\n",
        "                                 [0.5, 0.5, 0.0]\n",
        "  ]))\n",
        "  print(acts)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 0]\n",
            "[1 2 1]\n",
            "[1 2 0]\n",
            "[1 2 1]\n",
            "[1 2 0]\n",
            "[1 2 0]\n",
            "[0 2 0]\n",
            "[1 2 0]\n",
            "[1 2 0]\n",
            "[1 2 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HikN9fBJo_JT"
      },
      "source": [
        "## DQNAgent \n",
        "\n",
        "This class is appluicable in Q-learning when the action space is not very large which covers Atari games and lots of classical problems. \n",
        "\n",
        "A sample use case is a DQNAgent that takes in a batch of observations on input applies the network on them to get Q-values and then uses the provided ActionSelector to convert Q-values to indices of actions. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eISV_Pdgi_Mz",
        "outputId": "8e1f21af-d9bd-4c0f-d94c-dd36d4836efa"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class DQNNet(nn.Module):\n",
        "       def __init__(self, actions):\n",
        "           super(DQNNet, self).__init__()\n",
        "           self.actions = actions\n",
        "\n",
        "       def forward(self, x):\n",
        "           return torch.eye(x.size()[0], self.actions)\n",
        "\n",
        "net = DQNNet(actions=3)\n",
        "net(torch.zeros(2,10))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3VY5Wylp0zo",
        "outputId": "033a743a-0100-4300-a306-fb411ef5c4ce"
      },
      "source": [
        "# An input which is the batch of two observations and each having 5 values \n",
        "# The agent returned a tuple of two objects\n",
        "# The first one is the actions that the agent is supposed to take for each batch\n",
        "# The second one is the internal states of the agent and as the agent is stateless they return None\n",
        "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector)\n",
        "agent(torch.zeros(2,5))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), [None, None])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-1ZRRohqatl",
        "outputId": "9a593292-5329-4d22-8871-82d9a4113380"
      },
      "source": [
        "# This returns the epsilon selector when the value of epsilon is 1\n",
        "# It just shows the random actions the agent will take in the environment \n",
        "agent2 = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector_epsilon)\n",
        "agent2(torch.zeros(10,5))[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 2, 2, 0, 1, 1, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJm5nhGYrmk2",
        "outputId": "d27eb189-48f3-4103-d58d-5b39f71a8252"
      },
      "source": [
        "# The epsilon value can be changed on the fly during training \n",
        "selector_epsilon.epsilon = 0.5\n",
        "agent2(torch.zeros(10,5))[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 2, 0, 0, 0, 1, 2, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4oSZR-bsJMv"
      },
      "source": [
        "class PolicyNet(nn.Module):\n",
        "  def __init__(self, actions):\n",
        "    super(PolicyNet, self).__init__()\n",
        "    self.actions = actions\n",
        "  def forward(self,x):\n",
        "    # Now we produce the tensor with first two actions having the same logit scores\n",
        "    shape = (x.size()[0], self.actions)\n",
        "    res = torch.zeros(shape, dtype=torch.float32)\n",
        "    res[:, 0] = 1\n",
        "    res[:, 1] = 1\n",
        "    return res"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmj85w4Bsb6K",
        "outputId": "64f3b2d4-c45a-4e9a-8efc-88685839999d"
      },
      "source": [
        "net = PolicyNet(actions=5)\n",
        "net(torch.zeros(6,10))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKHzz-_btBqt",
        "outputId": "685117c5-dc1e-4b29-b5b5-8075f804b118"
      },
      "source": [
        "# ProbabilityActionSelector expects the probabilities to be normalized\n",
        "# So we use softmax on the networks outputs\n",
        "agent_prob = ptan.agent.PolicyAgent(model=net, action_selector=selector_prob, apply_softmax=True)\n",
        "agent(torch.zeros(6,5))[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJFra6fjd7lF"
      },
      "source": [
        "### Experience source \n",
        "\n",
        "The experience source classes take the agent instance and environment and provide you with step-by-step data from the trajectories. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRyVlh3RtW_I"
      },
      "source": [
        "from typing import List, Optional, Any, Tuple\n",
        "import ptan\n",
        "import gym\n",
        "\n",
        "class ToyEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    super(ToyEnv, self).__init__()\n",
        "    self.observation_space = gym.spaces.Discrete(n=5)\n",
        "    self.action_space = gym.spaces.Discrete(n=3)\n",
        "    self.step_index = 0\n",
        "  \n",
        "  def reset(self):\n",
        "    self.step_index = 0\n",
        "    return self.step_index\n",
        "  \n",
        "  def setup(self, action):\n",
        "    is_done = self.step_index == 10\n",
        "    if is_done:\n",
        "      return self.step_index % self.observation_space.n, 0.0, is_done, {} \n",
        "    self.step_index += 1\n",
        "    return self.step_index % self.observation_space.n, float(action), self.step_index == 10, {}\n",
        "\n",
        "class DullAgent(ptan.agent.BaseAgent):\n",
        "       \"\"\"\n",
        "       Agent always returns the fixed action\n",
        "       \"\"\"\n",
        "       def __init__(self, action: int):\n",
        "           self.action = action\n",
        "       def __call__(self, observations: List[Any], state: Optional[List] = None) -> Tuple[List[int], Optional[List]]:\n",
        "           return [self.action for _ in observations], state\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmhxH9dBglf6"
      },
      "source": [
        "# The ExperieceSource class\n",
        "# Outputs:\n",
        "# (Experience(state=0, action=1, reward=1.0, done=False),\n",
        "#  Experience(state=1, action=1, reward=1.0, done=False))\n",
        "#  (Experience(state=1, action=1, reward=1.0, done=False),\n",
        "#  Experience(state=2, action=1, reward=1.0, done=False))\n",
        "#  (Experience(state=2, action=1, reward=1.0, done=False),\n",
        "#  Experience(state=3, action=1, reward=1.0, done=False))\n",
        "# On every iteration ExperienceOusrce returns a piece of the agent's trajectory in environment communication \n",
        "\n",
        "import gym\n",
        "\n",
        "env = ToyEnv()\n",
        "agent = DullAgent(action=2)\n",
        "\n",
        "exp_source = ptan.experience.ExperienceSource(env=env,agent=agent, steps_count=2)\n",
        "\n",
        "for idx, exp in enumerate(exp_source):\n",
        "  if idx > 2:\n",
        "    break\n",
        "  print(exp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhYPqbwqiG73"
      },
      "source": [
        "# The class ExperienceSource provides us with full subtrajectories of the given length as the list of (s, a, r) objects. \n",
        "# The next state, s', is returned in the next tuple, which is not always convenient. For example, in DQN training, we want to have tuples (s, a,r, s') at once to do one-step Bellman approximation during the training. \n",
        "# In addition, some extension of DQN, like n-step DQN, might want to collapse longer sequences of observations into (first-state, action, total-reward-for-n-steps, state-after-step-n).\n",
        "# To support this in a generic way, a simple subclass of ExperienceSource is implemented: ExperienceSourceFirstLast. \n",
        "# It accepts almost the same arguments in the constructor, but returns different data.\n",
        "\n",
        "# Outputs:\n",
        "# ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
        "# ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
        "# ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
        "# ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
        "\n",
        "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent,gamma=1.0, steps_count=1)\n",
        "\n",
        "for idx, exp in enumerate(exp_source):\n",
        "  if idx > 2:\n",
        "    break\n",
        "  print(exp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SH5zgxKmZ-M"
      },
      "source": [
        "## Experience replay buffers\n",
        "\n",
        "in DQN we rarely deal with the immediate samples, as they are heavily correlated, which leads to instabilitiy in the training. Normally we have large replay buffers which are populated with experiece pieces. Then the buffer is samples to get the training batch. The replay buffer normally has a max capacity so old samples are pushed out when the replay buffer reaches the limit.\n",
        "\n",
        "1. ExperienceReplayBuffer: a simple replay buffer of predefined size with uniform sampling.\n",
        "\n",
        "2. PrioReplayBufferNaive: a simple, but not very efficient, prioritized replay buffer implementation. The complexity of sampling is O(n), which might become an issue with large buffers. This version has the advantage over the optimized class, having much easier code.\n",
        "\n",
        "3. PrioritizedReplayBuffer: uses segment trees for sampling, which makes the code cryptic, but with O(log(n)) sampling complexity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q-ZD14NnNHe"
      },
      "source": [
        "## The TargetNet Class\n",
        "\n",
        "TargetNet is a small but a useful class that allows us to synchronize two NNs of the same architecture.\n",
        "\n",
        "TargetNet supports two modes of such synchronization:\n",
        "\n",
        "1. sync(): weights from the source network are copied into the target network.\n",
        "   \n",
        "2. alpha_sync(): the source network's weights are blended into the target\n",
        "network with some alpha weight (between 0 and 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vpeVvmhno8n"
      },
      "source": [
        "## Iginite Helpers\n",
        "\n",
        "PTAN provides several small helpers to simplify integration with Ignite, which reside in the ptan.ignite package:\n",
        "\n",
        "1. EndOfEpisodeHandler: attached to the ignite.Engine, it emits an EPISODE_COMPLETED event, and tracks the reward and number of steps in the event in the engine's metrics. It also can emit an event when the average reward for the last episodes reaches the predefined boundary, which is supposed to be used to stop the training on some goal reward.\n",
        "\n",
        "2. EpisodeFPSHandler: tracks the number of interactions between the agent and environment that are performed and calculates performance metrics as frames per second. It also keeps the number of seconds passed since the start of the training.\n",
        " \n",
        "3. PeriodicEvents: emits corresponding events every 10, 100, or 1,000 training iterations. It is useful for reducing the amount of data being written into TensorBoard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X2QBTq9no4N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntOWS5xXlJhU"
      },
      "source": [
        "# The PTAN CartPole Solver\n",
        "\n",
        "# We create a simple feed-forward NN and target the NN epsilon-greedy action selector and DQNAgent\n",
        "# Then the experience source and replay buffer are created \n",
        "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "tgt_net = ptan.agent.TargetNet(net)\n",
        "selector = ptan.actions.ArgmaxActionSelector()\n",
        "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1, selector=selector)\n",
        "agent = ptan.agent.DQNAgent(net, selector)\n",
        "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
        "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\n",
        "\n",
        "while True:\n",
        "step += 1\n",
        "buffer.populate(1)\n",
        "# pop_rewards_steps() retuns the list of tuples with information about episodes completed since the last call to the method\n",
        "  for reward, steps in exp_source.pop_rewards_steps():\n",
        "      episode += 1\n",
        "      print(\"%d: episode %d done, reward=%.3f, epsilon=%.2f\" % (\n",
        "          step, episode, reward, selector.epsilon))\n",
        "      solved = reward > 150\n",
        "  if solved:\n",
        "      print(\"Congrats!\")\n",
        "      break\n",
        "  if len(buffer) < 2*BATCH_SIZE:\n",
        "    continue\n",
        "  batch = buffer.sample(BATCH_SIZE)\n",
        "  states_v, actions_v, tgt_q_v = unpack_batch(\n",
        "           batch, tgt_net.target_model, GAMMA)\n",
        "  optimizer.zero_grad()\n",
        "  q_v = net(states_v)\n",
        "  q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "  loss_v = F.mse_loss(q_v, tgt_q_v)\n",
        "  loss_v.backward()\n",
        "  optimizer.step()\n",
        "  selector.epsilon *= EPS_DECAY\n",
        "  if step % TGT_NET_SYNC == 0:\n",
        "      tgt_net.sync()\n",
        "\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}